{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgDsqzACCYrvpUCqRFary8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haturusinghe/cnn-lp-detection/blob/main/LRPNET_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Drive"
      ],
      "metadata": {
        "id": "F3j0KPcDH5u3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BwErFHgHpnk",
        "outputId": "300bd33c-0e7c-48be-b1ea-5a05b49a6ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = '/content/drive/MyDrive/npr/'\n",
        "LPRNET_DIR = BASE_DIR + 'LPRNET/'"
      ],
      "metadata": {
        "id": "_45leonmHw0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "ipAXZNEQITYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "8Xujl-opASo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import math\n",
        "import numpy as np\n",
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "c8dRaqrOIUVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import Sequential, Model\n",
        "from keras.layers import Activation,Conv2D, BatchNormalization, MaxPool2D,Softmax,Dropout, Input, ReLU, \\\n",
        "    Concatenate, Dense, Flatten\n",
        "from keras.models import load_model"
      ],
      "metadata": {
        "id": "rAhAWsA2vdQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import random"
      ],
      "metadata": {
        "id": "r_rbNI5N4C5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import editdistance"
      ],
      "metadata": {
        "id": "FFEsYpyP6QZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from data_aug import data_augmentation"
      ],
      "metadata": {
        "id": "2cd2Gjpf6lIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time"
      ],
      "metadata": {
        "id": "nUun7tDMpBJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variables"
      ],
      "metadata": {
        "id": "OdeN5tXEICxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHARS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n",
        "CHARS_SIN = \"ගුවන්යුහනාහශ්‍රී\"\n",
        "CHARS = CHARS + CHARS_SIN\n",
        "NUM_CLASS = len(set(CHARS))+1\n",
        "tf.compat.v1.enable_eager_execution()"
      ],
      "metadata": {
        "id": "tSmbsXfrIEJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = LPRNET_DIR +\"train\"  #path to the train directory\n",
        "val_dir = LPRNET_DIR + \"valid\"  #path to the validation directory"
      ],
      "metadata": {
        "id": "VKmqRpeKiM-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_epochs = 100    #number of training epochs  #default = 1000\n",
        "batch_size =  2  #default = 8  #batch size (train)\n",
        "val_batch_size =  2  #default = 4  #Validation batch size\n",
        "lr = 1e-3  #  #default = 1e-3  #initial learning rate\n",
        "decay_steps = 500  #  #default = 500  #learning rate decay rate\n",
        "decay_rate =   0.995  #learning rate decay rate  #default = 0.995\n",
        "staircase = \"smooth\"  #learning rate decay on step (default:smooth)\n",
        "\n",
        "pretrained = None  #pretrained model location\n",
        "saved_dir= \"saved_models\" #default = \"saved_models\"  #folder for saving models"
      ],
      "metadata": {
        "id": "4NOBrCy_rSo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "XccFSWeDK6z0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Util Variables"
      ],
      "metadata": {
        "id": "pCeFcL9w1x7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = [94, 24]\n",
        "CH_NUM = 3\n",
        "\n",
        "CHARS_DICT = {char:i for i, char in enumerate(set(CHARS))}\n",
        "DECODE_DICT = {i:char for i, char in enumerate(set(CHARS))}"
      ],
      "metadata": {
        "id": "tNw1i2jdK8CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "Ah71WXlR2QcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_label(label, char_dict):\n",
        "    encode = [char_dict[c] for c in label]\n",
        "    return encode"
      ],
      "metadata": {
        "id": "WzDTksbr2UVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_tuple_from(sequences, dtype=np.int32):\n",
        "    \"\"\"\n",
        "    Create a sparse representention of x.\n",
        "    Args:\n",
        "        sequences: a list of lists of type dtype where each element is a sequence\n",
        "    Returns:\n",
        "        A tuple with (indices, values, shape)\n",
        "    \"\"\"\n",
        "    indices = []\n",
        "    values = []\n",
        "\n",
        "    for n, seq in enumerate(sequences):\n",
        "        indices.extend(zip([n] * len(seq), range(len(seq))))\n",
        "        values.extend(seq)\n",
        "\n",
        "    indices = np.asarray(indices, dtype=np.int64)\n",
        "    values = np.asarray(values, dtype=dtype)\n",
        "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1] + 1], dtype=np.int64)\n",
        "\n",
        "    return indices, values, shape"
      ],
      "metadata": {
        "id": "CGQDF0Xy2SCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Iterator Class for Training"
      ],
      "metadata": {
        "id": "GAqcThRQ2eXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataIterator:\n",
        "    def __init__(self, img_dir, batch_size, runtime_generate=False):\n",
        "        self.img_dir = img_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.channel_num = CH_NUM\n",
        "        self.img_w, self.img_h = IMG_SIZE\n",
        "\n",
        "        if runtime_generate:\n",
        "            self.generator = None\n",
        "        else:\n",
        "            self.init()\n",
        "\n",
        "    def init(self):\n",
        "        self.filenames = []\n",
        "        self.labels = []\n",
        "        fs = os.listdir(self.img_dir)\n",
        "        for filename in fs:\n",
        "            self.filenames.append(filename)\n",
        "            label = filename.split('_')[0] # format: [label]_[random number].jpg\n",
        "            # print(label)\n",
        "            label = encode_label(label, CHARS_DICT)\n",
        "            self.labels.append(label)\n",
        "        self.sample_num = len(self.labels)\n",
        "        self.labels = np.array(self.labels)\n",
        "        self.random_index = list(range(self.sample_num))\n",
        "        random.shuffle(self.random_index)\n",
        "        self.cur_index = 0\n",
        "\n",
        "    def next_sample_ind(self):\n",
        "        ret = self.random_index[self.cur_index]\n",
        "        self.cur_index += 1\n",
        "        if self.cur_index >= self.sample_num:\n",
        "            self.cur_index = 0\n",
        "            random.shuffle(self.random_index)\n",
        "        return ret\n",
        "\n",
        "    def next_batch(self):\n",
        "\n",
        "        batch_size = self.batch_size\n",
        "        images = np.zeros([batch_size, self.img_h, self.img_w, self.channel_num])\n",
        "        labels = []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            sample_ind = self.next_sample_ind()\n",
        "            fname = self.filenames[sample_ind]\n",
        "            img = cv2.imread(os.path.join(self.img_dir, fname))\n",
        "            #img = data_augmentation(img)\n",
        "            img = cv2.resize(img, (self.img_w, self.img_h))\n",
        "            images[i] = img\n",
        "\n",
        "            labels.append(self.labels[sample_ind])\n",
        "\n",
        "        sparse_labels = sparse_tuple_from(labels)\n",
        "\n",
        "        return images, sparse_labels, labels\n",
        "\n",
        "    def next_test_batch(self):\n",
        "\n",
        "        start = 0\n",
        "        end = self.batch_size\n",
        "        is_last_batch = False\n",
        "\n",
        "        while not is_last_batch:\n",
        "            if end >= self.sample_num:\n",
        "                end = self.sample_num\n",
        "                is_last_batch = True\n",
        "\n",
        "            #print(\"s: {} e: {}\".format(start, end))\n",
        "\n",
        "            cur_batch_size = end-start\n",
        "            images = np.zeros([cur_batch_size, self.img_h, self.img_w, self.channel_num])\n",
        "\n",
        "            for j, i in enumerate(range(start, end)):\n",
        "                fname = self.filenames[i]\n",
        "                img = cv2.imread(os.path.join(self.img_dir, fname))\n",
        "                img = cv2.resize(img, (self.img_w, self.img_h))\n",
        "                images[j, ...] = img\n",
        "\n",
        "            labels = self.labels[start:end, ...]\n",
        "            sparse_labels = sparse_tuple_from(labels)\n",
        "\n",
        "            start = end\n",
        "            end += self.batch_size\n",
        "\n",
        "            yield images, sparse_labels, labels\n",
        "\n",
        "    def next_gen_batch(self):\n",
        "\n",
        "        batch_size = self.batch_size\n",
        "        imgs, labels = self.generator.generate_images(batch_size)\n",
        "        labels = [encode_label(label, CHARS_DICT) for label in labels]\n",
        "\n",
        "        images = np.zeros([batch_size, self.img_h, self.img_w, self.channel_num])\n",
        "        for i, img in enumerate(imgs):\n",
        "            img = data_augmentation(img)\n",
        "            img = cv2.resize(img, (self.img_w, self.img_h))\n",
        "            images[i, ...] = img\n",
        "\n",
        "        sparse_labels = sparse_tuple_from(labels)\n",
        "\n",
        "        return images, sparse_labels, labels"
      ],
      "metadata": {
        "id": "fnXmLRJ-2hF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "jo_hJrcjK3TV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv2D_batchnorm(*args, **kwargs):\n",
        "    return Sequential([Conv2D(*args, **kwargs),\n",
        "                   BatchNormalization(),\n",
        "                   ReLU()])"
      ],
      "metadata": {
        "id": "0idZUKdNK4om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LPRNet:\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        pattern_size=128,\n",
        "        dropout=0.5,\n",
        "        input_shape=(24, 94, 3),\n",
        "        include_STN=False,\n",
        "    ):\n",
        "        self.num_classes = num_classes\n",
        "        self.pattern_size = pattern_size\n",
        "        self.dropout = dropout\n",
        "        self.input_shape = input_shape\n",
        "        self.input_block = self.mixed_input_block\n",
        "        self.basic_block = self.basic_blocks\n",
        "        self.model = self._build()\n",
        "\n",
        "    def _build(self):\n",
        "        inputs = Input(self.input_shape)\n",
        "        x = self.input_block()(inputs)\n",
        "        x = self.basic_block(x.get_shape().as_list()[3], 256)(x)\n",
        "        x = self.convolution_block(x.get_shape().as_list()[3], 256, 2)(x)\n",
        "\n",
        "        x = Dropout(self.dropout)(x)\n",
        "        x = conv2D_batchnorm(256, [4, 1])(x)\n",
        "        x = Dropout(self.dropout)(x)\n",
        "\n",
        "        classes = conv2D_batchnorm(self.num_classes, [1, 13], padding=\"same\")(x)\n",
        "        pattern = Conv2D(128, [1, 1])(classes)\n",
        "        x = Concatenate()([classes, pattern])\n",
        "        outs = conv2D_batchnorm(self.num_classes, [1, 1], padding=\"same\")(x)\n",
        "\n",
        "        return Model(inputs=inputs, outputs=outs)\n",
        "\n",
        "    @staticmethod\n",
        "    def basic_blocks(channel_in, channel_out):\n",
        "        return Sequential(\n",
        "            [\n",
        "                conv2D_batchnorm(channel_out // 4, [1, 1], padding=\"same\"),\n",
        "                conv2D_batchnorm(channel_out // 4, [3, 1], padding=\"same\"),\n",
        "                conv2D_batchnorm(channel_out // 4, [1, 3], padding=\"same\"),\n",
        "                conv2D_batchnorm(channel_out // 4, [1, 1], padding=\"same\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def mixed_input_block(self):\n",
        "        return Sequential(\n",
        "            [\n",
        "                conv2D_batchnorm(64, [3, 3], padding=\"same\"),\n",
        "                MaxPool2D([3, 3], strides=[1, 1]),\n",
        "                self.basic_block(64, 128),\n",
        "                MaxPool2D([3, 3], strides=[2, 1]),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    # Convolution block for CNN\n",
        "    def convolution_block(self, channel_in, channel_out, stride):\n",
        "        return Sequential(\n",
        "            [\n",
        "                self.basic_block(channel_in, channel_out),\n",
        "                MaxPool2D([3, 3], strides=(stride, 1)),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def train(self):\n",
        "        raise NotImplemented\n",
        "\n",
        "    def predict(self, x, classnames):\n",
        "        pred = self.model.predict(x)\n",
        "        return pred\n",
        "\n",
        "    def decode_pred(self, pred, classnames):\n",
        "        samples, times = pred.shape[:2]\n",
        "        input_length = tf.convert_to_tensor([times] * samples)\n",
        "        decodeds, logprobs = tf.keras.backend.ctc_decode(\n",
        "            pred, input_length, greedy=True, beam_width=100, top_paths=1\n",
        "        )\n",
        "        decodeds = np.array(decodeds[0])\n",
        "\n",
        "        results = []\n",
        "        for d in decodeds:\n",
        "            text = []\n",
        "            for idx in d:\n",
        "                if idx == -1:\n",
        "                    break\n",
        "                text.append(classnames[idx])\n",
        "            results.append(\"\".join(text).encode(\"utf-8\"))\n",
        "        return results\n",
        "\n",
        "    def save_weights(self, filepath):\n",
        "        self.model.save_weights(filepath)\n",
        "\n",
        "    def load_weights(self, filepath):\n",
        "        self.model.load_weights(filepath)\n",
        "\n",
        "    def save(self, filepath):\n",
        "        self.model.save(filepath)\n",
        "\n",
        "    def summary(self):\n",
        "        self.model.summary()\n"
      ],
      "metadata": {
        "id": "LspEYFnvzKqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate"
      ],
      "metadata": {
        "id": "RNlTM2VQK46q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluator:\n",
        "\n",
        "    def __init__(self,val_gen, net, class_names,val_batch_len,batch_size):\n",
        "        self.net = net\n",
        "        self.val_gen = val_gen\n",
        "        self.class_names = class_names\n",
        "        self.batch_size = batch_size\n",
        "        self.val_gen = val_gen\n",
        "        self.val_batch_len = val_batch_len\n",
        "\n",
        "    def _average(self, values):\n",
        "        if len(values) == 1:\n",
        "            return values[0]\n",
        "        return(np.sum(values )/self.val_batch_len)\n",
        "        # return (np.sum(values[:-1] * self.batch_size) + values[-1] * last_batch_size) / len(self.loader)\n",
        "\n",
        "\n",
        "    def _decode_label(self,labels):\n",
        "        results = []\n",
        "        for d in labels:\n",
        "            text = []\n",
        "            for idx in d:\n",
        "                if idx == -1:\n",
        "                    break\n",
        "                text.append(self.class_names[idx])\n",
        "            results.append(''.join(text).encode('utf-8'))\n",
        "        return results\n",
        "\n",
        "    def decode_pred(self,pred ):\n",
        "        samples, times = pred.shape[:2]\n",
        "        input_length = tf.convert_to_tensor([times] * samples)\n",
        "        decodeds, logprobs = tf.keras.backend.ctc_decode(pred, input_length, greedy=True, beam_width=100, top_paths=1)\n",
        "        decodeds = np.array(decodeds[0])\n",
        "\n",
        "        results = []\n",
        "        for d in decodeds:\n",
        "            text = []\n",
        "            for idx in d:\n",
        "                if idx == -1:\n",
        "                    break\n",
        "                text.append(self.class_names[idx])\n",
        "            results.append(''.join(text).encode('utf-8'))\n",
        "        return results\n",
        "\n",
        "    def _calc_CER_and_WER(self, label_texts, decoded_texts):\n",
        "        ed = []\n",
        "        WER = 0\n",
        "        for label, pred in zip(label_texts, decoded_texts):\n",
        "            print(\"label \\t {} \\t prediction \\t {}\".format(label,pred))\n",
        "            cer = editdistance.eval(label, pred)\n",
        "            ed.append(cer)\n",
        "            if cer != 0:\n",
        "                WER += 1\n",
        "        WER /= len(label_texts)\n",
        "        CER = sum(ed) / len(label_texts)\n",
        "        return CER, WER\n",
        "\n",
        "    def _print_result(self, loss, CER, WER):\n",
        "        print(\"Number of samples in test set: {}\\n\"\n",
        "              \"mean loss: {}\\n\"\n",
        "              \"mean CER: {}\\n\"\n",
        "              \"WER: {}\\n\".format(self.val_batch_len * self.batch_size,\n",
        "                                 loss,\n",
        "                                 CER,\n",
        "                                 WER\n",
        "                                 )\n",
        "              )\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.losses, self.CERs, self.WERs = [],[],[]\n",
        "\n",
        "        for val_batch in range(self.val_batch_len):\n",
        "            val_inputs, val_targets, val_labels = self.val_gen.next_batch()\n",
        "            val_inputs = val_inputs.astype('float32')\n",
        "            val_targets = tf.SparseTensor(val_targets[0], val_targets[1], val_targets[2])\n",
        "            logits = self.net.model(val_inputs, training = False)\n",
        "            logits = tf.reduce_mean(logits, axis = 1)\n",
        "            decoded_texts = self.decode_pred(logits)\n",
        "            label_texts = self._decode_label(val_labels)\n",
        "            CER,WER = self._calc_CER_and_WER(label_texts, decoded_texts)\n",
        "\n",
        "            logits_shape = tf.shape(logits)\n",
        "            seq_len = tf.fill([logits_shape[0]],logits_shape[1])\n",
        "            logits = tf.transpose(logits, (1,0,2))\n",
        "            loss_value = tf.reduce_mean(tf.compat.v1.nn.ctc_loss(labels = val_targets, inputs = logits, sequence_length = seq_len ))\n",
        "            # print(\"Loss: {} - CER: {}, WER:{}\\n\".format(float(loss_value),CER,WER))\n",
        "            self.losses.append(float(loss_value))\n",
        "            self.CERs.append(CER)\n",
        "            self.WERs.append(WER)\n",
        "        loss = self._average(self.losses)\n",
        "        cer = self._average(self.CERs)\n",
        "        wer = self._average(self.WERs)\n",
        "        self._print_result(loss,cer,wer)\n",
        "        return(loss)"
      ],
      "metadata": {
        "id": "9F6w3LZi6Whl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "unILQvFeH7og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "\n",
        "\t#Initiate the Neural Network\n",
        "\t#Defined in model.py\n",
        "\tnet = LPRNet(NUM_CLASS)\n",
        "\n",
        "\n",
        "\t#initialize the custom data generator\n",
        "\t#Defined in utils.py\n",
        "\ttrain_gen = DataIterator(img_dir=train_dir, batch_size = batch_size)\n",
        "\tval_gen = DataIterator(img_dir=val_dir,batch_size = val_batch_size)\n",
        "\n",
        "\t#variable intialization used for custom training loop\n",
        "\ttrain_len = len(next(os.walk(train_dir))[2])\n",
        "\tval_len = len(next(os.walk(val_dir))[2])\n",
        "\tprint(\"Train Len is\", train_len)\n",
        "\n",
        "\n",
        "\tif batch_size ==1:\n",
        "\t\tBATCH_PER_EPOCH = train_len\n",
        "\telse:\n",
        "\t\tBATCH_PER_EPOCH = int(math.ceil(train_len/batch_size))\n",
        "\n",
        "\t#initialize tensorboard\n",
        "\ttensorboard = keras.callbacks.TensorBoard(log_dir = 'tmp/my_tf_logs',histogram_freq = 0,\n",
        "\t\tbatch_size = batch_size, write_graph = True)\n",
        "\n",
        "\tval_batch_len = int(math.floor(val_len / val_batch_size))\n",
        "\tevaluator = Evaluator(val_gen,net, CHARS,val_batch_len, val_batch_size) #Check evaluate.py\n",
        "\tbest_val_loss = float(\"inf\")\n",
        "\n",
        "\t#if a pretrained model is available, load weights from it\n",
        "\t#if pretrained:\n",
        "\t#\tnet.load_weights(pretrained)\n",
        "\n",
        "\n",
        "\tmodel = net.model\n",
        "\ttensorboard.set_model(model)\n",
        "\n",
        "\t#initialize the learning rate\n",
        "\tlearning_rate = keras.optimizers.schedules.ExponentialDecay(lr,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdecay_steps=decay_steps,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdecay_rate=decay_rate,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstaircase=staircase)\n",
        "\n",
        "\t#define training optimizer\n",
        "\toptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\tprint('Training ...')\n",
        "\ttrain_loss = 0\n",
        "\n",
        "\t#starting the training loop\n",
        "\tfor epoch in range(train_epochs):\n",
        "\n",
        "\t\tprint(\"Start of epoch {} / {}\".format(epoch,train_epochs))\n",
        "\n",
        "\t\t#zero out the train_loss and val_loss at the beginning of every loop\n",
        "\t\t#This helps us track the loss value for every epoch\n",
        "\t\ttrain_loss = 0\n",
        "\t\tval_loss = 0\n",
        "\t\tstart_time = time.time()\n",
        "\n",
        "\t\tfor batch in range(BATCH_PER_EPOCH):\n",
        "\t\t\tprint(\"batch {}/{}\".format(batch, BATCH_PER_EPOCH))\n",
        "\t\t\t#get a batch of images/labels\n",
        "\t\t\t#the labels have to be put into sparse tensor to feed into tf.nn.ctc_loss\n",
        "\t\t\ttrain_inputs,train_targets,train_labels = train_gen.next_batch()\n",
        "\t\t\ttrain_inputs = train_inputs.astype('float32')\n",
        "\n",
        "\t\t\ttrain_targets = tf.SparseTensor(train_targets[0],train_targets[1],train_targets[2])\n",
        "\n",
        "\n",
        "\t\t# Open a GradientTape to record the operations run\n",
        "\t\t# during the forward pass, which enables auto-differentiation.\n",
        "\t\t\twith tf.GradientTape() as tape:\n",
        "\n",
        "\t\t\t\t#get model outputs\n",
        "\t\t\t\tlogits = model(train_inputs,training = True)\n",
        "\n",
        "\t\t\t\t#next we pass the model outputs into the ctc loss function\n",
        "\t\t\t\tlogits = tf.reduce_mean(logits, axis = 1)\n",
        "\t\t\t\tlogits_shape = tf.shape(logits)\n",
        "\t\t\t\tcur_batch_size = logits_shape[0]\n",
        "\t\t\t\ttimesteps = logits_shape[1]\n",
        "\t\t\t\tseq_len = tf.fill([cur_batch_size],timesteps)\n",
        "\t\t\t\tlogits = tf.transpose(logits,(1,0,2))\n",
        "\t\t\t\tctc_loss = tf.compat.v1.nn.ctc_loss(labels = train_targets, inputs = logits, sequence_length = seq_len)\n",
        "\t\t\t\tloss_value =tf.reduce_mean(ctc_loss)\n",
        "\n",
        "\n",
        "\n",
        "\t\t\t#Calculate Gradients and Update it\n",
        "\t\t\tgrads = tape.gradient(ctc_loss, model.trainable_weights,unconnected_gradients=tf.UnconnectedGradients.NONE)\n",
        "\t\t\toptimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\t\t\ttrain_loss += float(loss_value)\n",
        "\n",
        "\n",
        "\t\ttim = time.time() - start_time\n",
        "\n",
        "\t\tprint(\"Train loss {}, time {} \\n\".format(float(train_loss/BATCH_PER_EPOCH),tim))\n",
        "\t\t#run a validation loop in every 25 epoch\n",
        "\t\tif epoch != 0 and epoch%25 == 0:\n",
        "\t\t\tval_loss = evaluator.evaluate()\n",
        "\t\t\t#if the validation loss is less the previous best validation loss, update the saved model\n",
        "\t\t\tif val_loss < best_val_loss:\n",
        "\t\t\t\tbest_val_loss = val_loss\n",
        "\t\t\t\tnet.save_weights(os.path.join(saved_dir, \"new_out_model_best.pb\"))\n",
        "\t\t\t\tprint(\"Weights updated in {}/{}\".format(saved_dir,\"new_out_model_best.pb\"))\n",
        "\n",
        "\t\t\telse:\n",
        "\t\t\t\tprint(\"Validation loss is greater than best_val_loss \")\n",
        "\n",
        "\t\t\t# if epoch %500 == 0:\n",
        "\t\t\t# \tnet.save(os.path.join(saved_dir, f\"new_out_model_last_{epoch}.pb\"))\n",
        "\n",
        "\n",
        "\n",
        "\tnet.save(os.path.join(saved_dir, \"new_out_model_last.pb\"))\n",
        "\tprint(\"Final Weights saved in {}/{}\".format(saved_dir, \"new_out_model_last.pb\"))\n",
        "\ttensorboard.on_train_end(None)"
      ],
      "metadata": {
        "id": "M14gi9m3H836"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test and Train Model"
      ],
      "metadata": {
        "id": "vuSZWigl7BSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "id": "r3Vyd_TF7D6E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a56d076e-ec02-4c19-aeac-8ae120d060a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Len is 6\n",
            "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ...\n",
            "Start of epoch 0 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 327.82908121744794, time 0.7231934070587158 \n",
            "\n",
            "Start of epoch 1 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 306.97023518880206, time 0.6387524604797363 \n",
            "\n",
            "Start of epoch 2 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 300.85740152994794, time 0.7008249759674072 \n",
            "\n",
            "Start of epoch 3 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 299.3066914876302, time 0.6992542743682861 \n",
            "\n",
            "Start of epoch 4 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 293.1197509765625, time 0.6849119663238525 \n",
            "\n",
            "Start of epoch 5 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 280.4199625651042, time 0.5874059200286865 \n",
            "\n",
            "Start of epoch 6 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 277.3896077473958, time 0.5903067588806152 \n",
            "\n",
            "Start of epoch 7 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 271.1135762532552, time 0.5882546901702881 \n",
            "\n",
            "Start of epoch 8 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 268.7672526041667, time 0.5814926624298096 \n",
            "\n",
            "Start of epoch 9 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 257.9996744791667, time 0.6152584552764893 \n",
            "\n",
            "Start of epoch 10 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 248.97994486490884, time 0.5896973609924316 \n",
            "\n",
            "Start of epoch 11 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 241.8472646077474, time 0.5869877338409424 \n",
            "\n",
            "Start of epoch 12 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 234.33655802408853, time 0.5632414817810059 \n",
            "\n",
            "Start of epoch 13 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 237.03590393066406, time 0.6528651714324951 \n",
            "\n",
            "Start of epoch 14 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 227.88602701822916, time 0.6745014190673828 \n",
            "\n",
            "Start of epoch 15 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 221.45072428385416, time 0.5771346092224121 \n",
            "\n",
            "Start of epoch 16 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 230.49932861328125, time 0.6113922595977783 \n",
            "\n",
            "Start of epoch 17 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 219.44732157389322, time 0.5964052677154541 \n",
            "\n",
            "Start of epoch 18 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 214.84065755208334, time 1.2174177169799805 \n",
            "\n",
            "Start of epoch 19 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 215.3394978841146, time 0.6108975410461426 \n",
            "\n",
            "Start of epoch 20 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 207.7960205078125, time 0.5843894481658936 \n",
            "\n",
            "Start of epoch 21 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 207.78337605794272, time 0.5931243896484375 \n",
            "\n",
            "Start of epoch 22 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 201.35814921061197, time 0.6448965072631836 \n",
            "\n",
            "Start of epoch 23 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 198.3683624267578, time 0.6688501834869385 \n",
            "\n",
            "Start of epoch 24 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 195.45970662434897, time 0.6549556255340576 \n",
            "\n",
            "Start of epoch 25 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 193.23638407389322, time 0.6510341167449951 \n",
            "\n",
            "label \t b'N\\xe0\\xb7\\x84W' \t prediction \t b'N'\n",
            "label \t b'K\\xe0\\xb7\\x84\\xe0\\xb7\\x94GD' \t prediction \t b'N'\n",
            "label \t b'N\\xe0\\xb7\\x84W' \t prediction \t b'N'\n",
            "label \t b'G\\xe0\\xb7\\x94W' \t prediction \t b'N'\n",
            "Number of samples in test set: 4\n",
            "mean loss: 204.07930755615234\n",
            "mean CER: 5.5\n",
            "WER: 1.0\n",
            "\n",
            "Weights updated in saved_models/new_out_model_best.pb\n",
            "Start of epoch 26 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 197.17113749186197, time 0.6755316257476807 \n",
            "\n",
            "Start of epoch 27 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 188.40928649902344, time 0.6680862903594971 \n",
            "\n",
            "Start of epoch 28 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 199.59866333007812, time 0.5648539066314697 \n",
            "\n",
            "Start of epoch 29 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 186.37920633951822, time 0.6539907455444336 \n",
            "\n",
            "Start of epoch 30 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 189.88827006022134, time 0.6532742977142334 \n",
            "\n",
            "Start of epoch 31 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 179.6903533935547, time 0.6560099124908447 \n",
            "\n",
            "Start of epoch 32 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 186.91582234700522, time 0.6804602146148682 \n",
            "\n",
            "Start of epoch 33 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 183.51935323079428, time 0.6673564910888672 \n",
            "\n",
            "Start of epoch 34 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 190.0482432047526, time 0.6701502799987793 \n",
            "\n",
            "Start of epoch 35 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 210.950927734375, time 0.657085657119751 \n",
            "\n",
            "Start of epoch 36 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 175.13980611165366, time 0.6622505187988281 \n",
            "\n",
            "Start of epoch 37 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 171.10464986165366, time 0.5687189102172852 \n",
            "\n",
            "Start of epoch 38 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 173.26121520996094, time 0.6180307865142822 \n",
            "\n",
            "Start of epoch 39 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 171.08289591471353, time 0.6784334182739258 \n",
            "\n",
            "Start of epoch 40 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 171.60144551595053, time 0.6047754287719727 \n",
            "\n",
            "Start of epoch 41 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 166.96842447916666, time 0.6026172637939453 \n",
            "\n",
            "Start of epoch 42 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 161.99161783854166, time 0.6732075214385986 \n",
            "\n",
            "Start of epoch 43 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 176.2200927734375, time 0.6785910129547119 \n",
            "\n",
            "Start of epoch 44 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 163.23120625813803, time 0.69773268699646 \n",
            "\n",
            "Start of epoch 45 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 179.1428476969401, time 0.6537716388702393 \n",
            "\n",
            "Start of epoch 46 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 195.70601399739584, time 0.6514904499053955 \n",
            "\n",
            "Start of epoch 47 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 171.06531778971353, time 0.7029120922088623 \n",
            "\n",
            "Start of epoch 48 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 158.1946767171224, time 0.6881453990936279 \n",
            "\n",
            "Start of epoch 49 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 166.36821492513022, time 0.6776478290557861 \n",
            "\n",
            "Start of epoch 50 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 165.40564473470053, time 0.6847293376922607 \n",
            "\n",
            "label \t b'N\\xe0\\xb7\\x84W' \t prediction \t b'N'\n",
            "label \t b'K\\xe0\\xb7\\x84\\xe0\\xb7\\x94GD' \t prediction \t b'N'\n",
            "label \t b'G\\xe0\\xb7\\x94W' \t prediction \t b'N'\n",
            "label \t b'N\\xe0\\xb7\\x84W' \t prediction \t b'N'\n",
            "Number of samples in test set: 4\n",
            "mean loss: 217.73008728027344\n",
            "mean CER: 5.5\n",
            "WER: 1.0\n",
            "\n",
            "Validation loss is greater than best_val_loss \n",
            "Start of epoch 51 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 166.45319112141928, time 0.5749938488006592 \n",
            "\n",
            "Start of epoch 52 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 163.22680155436197, time 0.5871596336364746 \n",
            "\n",
            "Start of epoch 53 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 186.90320841471353, time 0.5924146175384521 \n",
            "\n",
            "Start of epoch 54 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 183.16914876302084, time 0.6609315872192383 \n",
            "\n",
            "Start of epoch 55 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 153.8561808268229, time 0.6974437236785889 \n",
            "\n",
            "Start of epoch 56 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 160.45526631673178, time 0.6698691844940186 \n",
            "\n",
            "Start of epoch 57 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 148.9195760091146, time 0.618016242980957 \n",
            "\n",
            "Start of epoch 58 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 158.5860850016276, time 0.6378622055053711 \n",
            "\n",
            "Start of epoch 59 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 178.65215047200522, time 0.5935211181640625 \n",
            "\n",
            "Start of epoch 60 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 157.50929260253906, time 0.6038210391998291 \n",
            "\n",
            "Start of epoch 61 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 146.78569539388022, time 0.5889146327972412 \n",
            "\n",
            "Start of epoch 62 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 155.41455078125, time 0.6470901966094971 \n",
            "\n",
            "Start of epoch 63 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 154.09483846028647, time 0.6980955600738525 \n",
            "\n",
            "Start of epoch 64 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 175.95026652018228, time 0.6947760581970215 \n",
            "\n",
            "Start of epoch 65 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 153.50946553548178, time 0.6570947170257568 \n",
            "\n",
            "Start of epoch 66 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 142.0636444091797, time 0.646308422088623 \n",
            "\n",
            "Start of epoch 67 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 140.99453735351562, time 0.6183915138244629 \n",
            "\n",
            "Start of epoch 68 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 149.32085673014322, time 0.6364257335662842 \n",
            "\n",
            "Start of epoch 69 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 138.87377421061197, time 0.6638402938842773 \n",
            "\n",
            "Start of epoch 70 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 168.63880920410156, time 0.6945183277130127 \n",
            "\n",
            "Start of epoch 71 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 137.76445515950522, time 0.6507811546325684 \n",
            "\n",
            "Start of epoch 72 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 146.05721791585287, time 0.5920960903167725 \n",
            "\n",
            "Start of epoch 73 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 146.3498738606771, time 0.6310675144195557 \n",
            "\n",
            "Start of epoch 74 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 133.00489807128906, time 0.5889339447021484 \n",
            "\n",
            "Start of epoch 75 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 167.3609619140625, time 0.5950288772583008 \n",
            "\n",
            "label \t b'N\\xe0\\xb7\\x84W' \t prediction \t b'N'\n",
            "label \t b'N\\xe0\\xb7\\x84W' \t prediction \t b'N'\n",
            "label \t b'G\\xe0\\xb7\\x94W' \t prediction \t b'N'\n",
            "label \t b'K\\xe0\\xb7\\x84\\xe0\\xb7\\x94GD' \t prediction \t b'N'\n",
            "Number of samples in test set: 4\n",
            "mean loss: 228.0753173828125\n",
            "mean CER: 5.5\n",
            "WER: 1.0\n",
            "\n",
            "Validation loss is greater than best_val_loss \n",
            "Start of epoch 76 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 144.23308308919272, time 0.6011641025543213 \n",
            "\n",
            "Start of epoch 77 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 143.35095723470053, time 0.596428632736206 \n",
            "\n",
            "Start of epoch 78 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 141.26386006673178, time 0.6733355522155762 \n",
            "\n",
            "Start of epoch 79 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 140.06690979003906, time 0.6804203987121582 \n",
            "\n",
            "Start of epoch 80 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 139.02696736653647, time 0.6744217872619629 \n",
            "\n",
            "Start of epoch 81 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 140.6728769938151, time 0.7237098217010498 \n",
            "\n",
            "Start of epoch 82 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 137.09217580159506, time 0.6594140529632568 \n",
            "\n",
            "Start of epoch 83 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 127.60597229003906, time 0.5924785137176514 \n",
            "\n",
            "Start of epoch 84 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 137.07348124186197, time 0.6088371276855469 \n",
            "\n",
            "Start of epoch 85 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 155.8608652750651, time 0.6027512550354004 \n",
            "\n",
            "Start of epoch 86 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 127.21480305989583, time 0.5927708148956299 \n",
            "\n",
            "Start of epoch 87 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 126.27201334635417, time 0.6106932163238525 \n",
            "\n",
            "Start of epoch 88 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 158.0915273030599, time 0.6059417724609375 \n",
            "\n",
            "Start of epoch 89 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 156.19253540039062, time 0.7474493980407715 \n",
            "\n",
            "Start of epoch 90 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 121.06502532958984, time 0.7459180355072021 \n",
            "\n",
            "Start of epoch 91 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 156.03180948893228, time 0.6759862899780273 \n",
            "\n",
            "Start of epoch 92 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 154.90304056803384, time 0.720940351486206 \n",
            "\n",
            "Start of epoch 93 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 131.9226862589518, time 0.703920841217041 \n",
            "\n",
            "Start of epoch 94 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 120.81925964355469, time 0.7344298362731934 \n",
            "\n",
            "Start of epoch 95 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 129.02730560302734, time 0.6160345077514648 \n",
            "\n",
            "Start of epoch 96 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 117.32738494873047, time 0.6127798557281494 \n",
            "\n",
            "Start of epoch 97 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 119.42132059733073, time 0.7021784782409668 \n",
            "\n",
            "Start of epoch 98 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 118.22401682535808, time 0.6012856960296631 \n",
            "\n",
            "Start of epoch 99 / 100\n",
            "batch 0/3\n",
            "batch 1/3\n",
            "batch 2/3\n",
            "Train loss 149.14862569173178, time 0.5962069034576416 \n",
            "\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: saved_models/new_out_model_last.pb/assets\n",
            "Final Weights saved in saved_models/new_out_model_last.pb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other"
      ],
      "metadata": {
        "id": "sEN20TrhnEl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -zcvf archive-saved_models.tar.gz saved_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdcRIxUHnQHp",
        "outputId": "366f15a3-6c3d-42a6-87c2-ecbc191f9e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved_models/\n",
            "saved_models/new_out_model_best.pb.index\n",
            "saved_models/new_out_model_last.pb/\n",
            "saved_models/new_out_model_last.pb/variables/\n",
            "saved_models/new_out_model_last.pb/variables/variables.index\n",
            "saved_models/new_out_model_last.pb/variables/variables.data-00000-of-00001\n",
            "saved_models/new_out_model_last.pb/keras_metadata.pb\n",
            "saved_models/new_out_model_last.pb/saved_model.pb\n",
            "saved_models/new_out_model_last.pb/assets/\n",
            "saved_models/new_out_model_best.pb.data-00000-of-00001\n",
            "saved_models/checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "% zip -r saved_models.zip saved_models/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeMP34qxnEQP",
        "outputId": "1c0479b1-d227-4b58-d154-06f7c6edaf42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%zip` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict"
      ],
      "metadata": {
        "id": "_KmLJWepnzTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Function"
      ],
      "metadata": {
        "id": "_9jaLjLXoCaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classnames = CHARS"
      ],
      "metadata": {
        "id": "08htS56XoYBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_pred(pred,classnames):\n",
        "\tpred = np.mean(pred, axis = 1)\n",
        "\tsamples, times = pred.shape[:2]\n",
        "\tinput_length = tf.convert_to_tensor([times] * samples)\n",
        "\tdecodeds, logprobs = tf.keras.backend.ctc_decode(pred, input_length, greedy=True, beam_width=100, top_paths=1)\n",
        "\tdecodeds = np.array(decodeds[0])\n",
        "\n",
        "\tresults = []\n",
        "\tfor d in decodeds:\n",
        "\t\ttext = []\n",
        "\t\tfor idx in d:\n",
        "\t\t\tif idx == -1:\n",
        "\t\t\t\tbreak\n",
        "\t\t\ttext.append(classnames[idx])\n",
        "\t\tresults.append(''.join(text).encode('utf-8'))\n",
        "\treturn results"
      ],
      "metadata": {
        "id": "tyJtGxaRoERd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run():\n",
        "\t#load the KERAS model\n",
        "\tmodel = load_model(\"./saved_models/new_out_model_last.pb\")\n",
        "\tprint(\"Loaded Weights successfully\")\n",
        "\tprint(\"Actual Label \\t Predicted Label \")\n",
        "\tstart_time = time()\n",
        "\tcnt = 0\n",
        "\n",
        "\t#loop through all the files in the test folder\n",
        "\tfor filename in os.listdir(\"./images\"):\n",
        "\t\t#check if the file is an image\n",
        "\t\tif filename.endswith(\".jpg\") or filename.endswith(\".JPG\"):\n",
        "\t\t\t#read the file and preprocess it\n",
        "\t\t\tframe = cv2.imread(f\"./images/{filename}\")\n",
        "\t\t\timg = cv2.resize(frame, (94,24))\n",
        "\t\t\timg = np.expand_dims(img,axis = 0)\n",
        "\t\t\t#get the output sequence\n",
        "\t\t\tpred = model.predict(img)\n",
        "\n",
        "\t\t\t#decode the output sequence using keras ctc decode\n",
        "\t\t\tresult_ctc = decode_pred(pred, classnames)\n",
        "\t\t\toriginal_label = filename.split(\"_\")[0]\n",
        "\n",
        "\t\t\t#print the original sequence and the decoded sequence\n",
        "\t\t\tprint(original_label,\"\\t\",result_ctc[0].decode('utf-8'))\n",
        "\t\t\tcnt+=1\n",
        "\tprint(\"total time taken :\", time()-start_time)\n",
        "\n"
      ],
      "metadata": {
        "id": "TomBF0bqn0sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRvx1dwoovHW",
        "outputId": "0586297e-0518-4c10-ed69-25fdea299b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "Loaded Weights successfully\n",
            "Actual Label \t Predicted Label \n",
            "img1.jpg \t KNKNGNN\n",
            "total time taken : 0.43308115005493164\n"
          ]
        }
      ]
    }
  ]
}